---
layout: post
use-math: true
---

Various Optimization
======

## Introduction
### Gradient descent  

First order iterative optimization algorithm for finding a local minimum of a differentiable function
***

## Important Concept in Optimization
* Generalization
    How well the learned model will behave on unseen data.
* Under-fitting vs. Over-fitting
    > The phenomenon of fitting our data more closely than we fit the underlying distribution is called _overfitting_, vice versa.
* Cross Validation
* Bias-variance Trade-off
* BootStrapping
* Bagging & Boosting

***

## Gradient Descent Types
* Stochastic Gradient Descent
* Momentum
* Nesterov accelerated gradient
* Adagrad
* Adadelta
* RMSprop
* Adam

***

## Regularization Types
* Early Stopping
* Parameter norm penalty
* Data augmentation
* Noise robustness
* Label smoothing
* Dropout
* Batch normalization

