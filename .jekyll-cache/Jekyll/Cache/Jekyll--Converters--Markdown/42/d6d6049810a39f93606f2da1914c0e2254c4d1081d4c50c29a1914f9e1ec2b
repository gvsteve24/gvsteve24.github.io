I"º<h2 id="focused-on-trend-and-methodology">Focused on trend and methodology</h2>

<blockquote>
  <p>What I cannot create, I do not understand.
by Richard Feynman (1918-1988)</p>
</blockquote>

<ul>
  <li>What is the Generative Model ?</li>
  <li>GAN in what category in Generative model?</li>
</ul>

<h1 id="learning-a-generative-model">Learning a Generative Model</h1>
<ul>
  <li>Suppose we are given images of dogs.</li>
  <li>We want to learn a probability distribution $p(x)$ such that
    <ul>
      <li><strong>Generation</strong>: If we sample $x_{new}~p(x), x_{new}$ should look like a dog (sampling).</li>
      <li><strong>Density estimation</strong>: $p(x)$ should be high if $x$ looks like a dog, and low otherwise(anomaly detection).
        <ul>
          <li>Also known as, explicit models.</li>
        </ul>
      </li>
      <li><strong>Unsupervised representation learning</strong>: We should be able to learn what these images have in common, e.g., ears, tall, etc (feature learning)</li>
      <li>Then, how can we represent p(x)?</li>
    </ul>
  </li>
</ul>

<p>If we have values that are finite set, and are in our interest,</p>

<h1 id="basic-discrete-distributions">Basic Discrete Distributions</h1>
<ul>
  <li><strong>Bernoulli distribution</strong>:(biased) coin flip
    <ul>
      <li>$D=\{Heads,Tails\}$</li>
      <li>Specify $P(X = Heads) = p$. Then $P(X=Tails) = 1 - p$.</li>
      <li>Write: $X ~ Ber(p)$.</li>
    </ul>
  </li>
  <li><strong>Categorical distribution</strong>:(biased) m-sided dice
    <ul>
      <li>$D = \{1,\cdots,m\}$</li>
      <li>Specify $P(Y = i)=p_i$, such that $\sum^m_{i=1}p_i=1$.</li>
      <li>Write: $Y\sim Cat(p_1,\cdots,p_m)$</li>
    </ul>
  </li>
</ul>
:ET