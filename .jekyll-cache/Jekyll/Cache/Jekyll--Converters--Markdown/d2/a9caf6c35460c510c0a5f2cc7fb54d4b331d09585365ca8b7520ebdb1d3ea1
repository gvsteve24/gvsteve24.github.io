I"·<h1 id="latent-variable-models">Latent Variable Models</h1>
<h4 id="dkingma-variational-inference-and-deep-learning-a-new-synthesis-phd-thesis">D.Kingma, ‚ÄúVariational Inference and Deep Learning: A New Synthesis,‚Äù Ph.D. Thesis</h4>

<h1 id="question">Question</h1>
<h3 id="is-an-autoencoder-a-generative-model">Is an autoencoder a generative model?</h3>

<ul>
  <li>Many people knows variational autoencoder is a generative model. So is autoencoder.</li>
  <li>However, it‚Äôs not true.</li>
</ul>

<h1 id="variational-auto-encoder">Variational Auto-encoder</h1>

<ul>
  <li>Variational Inference (VI)
    <ul>
      <li>The goal of VI is to optimize the <strong>variational distribution</strong> that best matches the <strong>posterior distribution</strong>.
        <ul>
          <li><strong>Posterior distribution</strong>: $p_\theta(z|x)$</li>
          <li><strong>Variational distribution</strong>: $q_{\phi}(z|x)$</li>
        </ul>
      </li>
      <li>In particular, we want to find the <strong>variational distribution</strong> that minimizes the KL divergence between the true posterior.</li>
    </ul>
  </li>
  <li>
    <p>But how?</p>

    <p><a href="../images/VarAutoEncoder.png">!variational-autoencoder</a></p>
  </li>
</ul>
:ET