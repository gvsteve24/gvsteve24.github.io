I"F<h1 id="basics-of-recurrent-neural-networks-rnns">Basics of Recurrent Neural Networks (RNNs)</h1>

<h2 id="type-of-rnns">Type of RNNs</h2>
<hr />
<ul>
  <li>
    <h3 id="basic-structure">Basic structure</h3>
    <p><img src="../images/unrolled recurrent neural network.PNG" alt="unrolled neural network" /></p>
  </li>
</ul>

<h2 id="recurrent-neural-network">Recurrent Neural Network</h2>
<hr />
<ul>
  <li>
    <h3 id="inputs-and-outputs-of-rnns-rolled-version">Inputs and outputs of RNNs (rolled version)</h3>
    <ul>
      <li>
        <h4 id="we-usually-want-to-predict-a-vector-at-some-time-steps">We usually want to predict a vector at some time steps</h4>
        <p><img src="../images/rnn-diagram.png" alt="rnn diagram" /></p>
      </li>
    </ul>
  </li>
  <li>
    <h3 id="how-to-calculate-the-hidden-state-of-rnns">How to calculate the hidden state of RNNs</h3>
    <ul>
      <li>
        <h4 id="we-can-process-a-sequence-of-vectors-by-applying-a-recurrence-formula-at-every-time-step">We can process a sequence of vectors by applying a recurrence formula at every time step</h4>
      </li>
      <li>$h_{t-1}$: old hidden-state vector</li>
      <li>$x_t$: input vector at some time step</li>
      <li>$h_t$: new hidden-state vector</li>
      <li>$f_W$: RNN function with parameters W</li>
      <li>
        <p>$y_t$: output vector at time step t</p>

        <p>$h_t=f_W(h_{t-1}, x_t)$
  <img src="../images/rnn-diagram-2.png" alt="rnn diagram-2" /></p>
      </li>
      <li>
        <p><span style="color: red">Notice: The same function and the same set of parameters are used at every time step</span></p>

        <p>$h_t = f_W(h_{t-1}, x_t)$</p>
      </li>
      <li>The state consists of a single “hidden” vector <strong>h</strong></li>
    </ul>
  </li>
</ul>

<h1 id="types-of-rnns">Types of RNNs</h1>
<hr />
<ul>
  <li>
    <h3 id="one-to-one">One-to-one</h3>
    <ul>
      <li>Standard Neural Networks</li>
    </ul>
  </li>
  <li>
    <h3 id="one-to-many">One-to-many</h3>
    <ul>
      <li>Image Captioning</li>
    </ul>
  </li>
  <li>
    <h3 id="many-to-one">Many-to-one</h3>
    <ul>
      <li>Sentiment Classification</li>
    </ul>
  </li>
  <li>
    <h3 id="sequence-to-sequencetime-delayed">Sequence-to-sequence(time delayed)</h3>
    <ul>
      <li>Machine Translation</li>
    </ul>
  </li>
  <li>
    <h3 id="sequence-to-sequenceon-real-time">Sequence-to-sequence(on real time)</h3>
    <ul>
      <li>Video classification on frame level</li>
    </ul>
  </li>
</ul>

<h1 id="character-level-language-model">Character-level Language Model</h1>

<h2 id="character-level-language-model-1">Character-level Language Model</h2>
<hr />
<ul>
  <li>
    <h3 id="example-of-training-sequence-hello">Example of training sequence “hello”</h3>
    <ul>
      <li>Vocabulary: [h, e, l, o]</li>
      <li>Example training sequence: “hello”
  <img src="../images/many-to-many.png" alt="many-to-many case" /></li>
      <li>$h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t+b)$
  <img src="../images/hidden-state-t.png" alt="hidden-state-t" /></li>
      <li>$\text{Logit}=W_{hy}h_t+b$
  <img src="../images/logit-eq.png" alt="logit-eq" /></li>
      <li>At test-time, sample characters one at a time, feed back to model
  <img src="../images/feed-to-next.png" alt="feed-to-next" /></li>
    </ul>
  </li>
</ul>

<h2 id="backpropagation-through-time-bptt">Backpropagation through time (BPTT)</h2>
<hr />
<ul>
  <li>
    <h3 id="forward-through-entire-sequence-to-compute-loss-then-backward-through-entire-sequence-to-compute-gradient">Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient</h3>
    <p><img src="../images/bptt-1.png" alt="bptt-1" /></p>
    <h1 id="-run-forward-and-backward-through-chunks-of-the-sequence-instead-of-whole-sequence">### Run forward and backward through chunks of the sequence instead of whole sequence</h1>
    <p><img src="../images/bptt-chunk.png" alt="bptt-chunk" /></p>
  </li>
</ul>

<h1 id="long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</h1>

<h1 id="gated-recurrent-unit-gru">Gated Recurrent Unit (GRU)</h1>

<h2 id="long-short-term-memory-lstm-1">Long Short-Term Memory (LSTM)</h2>
<hr />
<ul>
  <li>
    <h3 id="core-idea-pass-cell-state-information-straightly-without-any-transformation">Core Idea: pass cell state information straightly without any transformation</h3>
  </li>
</ul>
:ET