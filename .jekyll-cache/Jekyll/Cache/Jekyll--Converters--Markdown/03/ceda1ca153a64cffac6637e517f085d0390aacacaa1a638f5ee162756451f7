I"<h2 id="focused-on-trend-and-methodology">Focused on trend and methodology</h2>

<blockquote>
  <p>What I cannot create, I do not understand.
by Richard Feynman (1918-1988)</p>
</blockquote>

<ul>
  <li>What is the Generative Model ?</li>
  <li>GAN in what category in Generative model?</li>
</ul>

<h1 id="learning-a-generative-model">Learning a Generative Model</h1>
<ul>
  <li>Suppose we are given images of dogs.</li>
  <li>We want to learn a probability distribution $p(x)$ such that
    <ul>
      <li><strong>Generation</strong>: If we sample $x_{new}~p(x), x_{new}$ should look like a dog (sampling).</li>
      <li><strong>Density estimation</strong>: $p(x)$ should be high if $x$ looks like a dog, and low otherwise(anomaly detection).
        <ul>
          <li>Also known as, explicit models.</li>
        </ul>
      </li>
      <li><strong>Unsupervised representation learning</strong>: We should be able to learn what these images have in common, e.g., ears, tall, etc (feature learning)</li>
      <li>Then, how can we represent $p(x)$?</li>
    </ul>
  </li>
</ul>

<p>If we have values that are finite set, and are in our interest,</p>

<h1 id="basic-discrete-distributions">Basic Discrete Distributions</h1>
<ul>
  <li><strong>Bernoulli distribution</strong>:(biased) coin flip
    <ul>
      <li>$D=\{Heads,Tails\}$</li>
      <li>Specify $P(X = Heads) = p$. Then $P(X=Tails) = 1 - p$.</li>
      <li>Write: $X \sim Ber(p)$.</li>
    </ul>
  </li>
  <li><strong>Categorical distribution</strong>:(biased) m-sided dice
    <ul>
      <li>$D = \{1,\cdots,m\}$</li>
      <li>Specify $P(Y = i)=p_i$, such that $\sum^m_{i=1}p_i=1$.</li>
      <li>Write: $Y\sim Cat(p_1,\cdots,p_m)$</li>
    </ul>
  </li>
</ul>

<h2 id="example">Example</h2>
<ul>
  <li>Modeling an RGB joint distribution ( of a single pixel)
    <ul>
      <li>$(r, g, b) \sim p(R, G, B)$</li>
      <li>
        <p>Number of cases?</p>

        <p>256 x 256 x 256</p>
      </li>
      <li>
        <p>How many parameters do we need to specify?</p>

        <p>255 x 255 x 255</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="example-simplified">Example (simplified)</h2>
<ul>
  <li>Suppose we have $X_1,\dots,X_n$ of $n$ binary pixels (a binary image).</li>
  <li>How many possible states?
  $2\times2\times\cdots\times2 = 2^n$</li>
  <li>Sampling from $p(x_1,\dots,x_n)$ generates an image.</li>
  <li>How many parameters to specify $p(x_1,\dots,x_n)$?</li>
</ul>

<h1 id="structure-through-independence">Structure Through Independence</h1>
<ul>
  <li>
    <p>What if $X_1,\dots,X_n$ are independent, then</p>

    <p>$p(x_1,\dots,x_n)=p(x_1)p(x_2)\cdots p(x_n)$</p>
  </li>
  <li>
    <p>How many possible states?</p>

    <p>$2^n$</p>
  </li>
  <li>
    <p>How many parameters to specify $p(x_1,\dots,x_n)$?</p>

    <p>$n$</p>
  </li>
  <li>
    <p>$2^n$ entries can be described by just n numbers! But this independence assumption is too strong to model useful distributions.</p>
  </li>
</ul>

<h1 id="conditional-independence">Conditional Independence</h1>
<ul>
  <li>Three important rules
    <ul>
      <li>
        <p><strong>Chaing rule</strong>:</p>

        <p>$p(x_1,\dots,x_n)=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\cdots p(x_n|x_1,\cdots,x_{n-1})$</p>
      </li>
      <li>
        <p><strong>Bayesâ€™ rule</strong>:</p>

        <table>
          <tbody>
            <tr>
              <td>$p(x</td>
              <td>y) = \fraction{p(x,y)}{p(y)}=\fraction{p(y</td>
              <td>x)p(x)}{p(y)}$</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ul>
  </li>
</ul>
:ET